#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Complexity
\begin_inset Newline newline
\end_inset

Combinatorial Syntactical Constraints
\begin_inset Newline newline
\end_inset

Syntactical Kinematics
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Date
11 January 2025
\end_layout

\begin_layout Abstract
Diary exploring how complexity is expressed, using tools from combinatorics
 and model theory.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Approximately 100 to 140 years ago, Boltzmann, Planck, Bose, Einstein, Dirac
 and Fermi considered the distributions to which their names are now attached.
 They considered collections of a large number of identical particles (photons,
 bosons, fermions) and how such collections behaved, thermodynamically.
 This lead to a microscopic description of the equation of state for a gas
 in a box, of a formula for black-body radiation, and foundations for some
 of the more exotic structures in nature.
\end_layout

\begin_layout Standard
Since then, there have been broad advances in dynamical systems and ergodic
 theory.
 There have also been advances in linguistics, computers, model theory,
 proof theory and, of course deep-learing neural-net large language models
 (LLM's) embedded in hyperdimensional vector spaces.
 Despite this, I have been unable to find, read about or personally formulate
 a general conception of how entropy and thermodynamics works in high-dimensiona
l spaces.
 Examples are abundant: What's the entropy of protein folding? What's the
 entropy of galaxy formation? What's the entropy of an immunoglobulin antibody
 binding site? A DNA coding region? What's the entropy of the English language?
 What's the entropy of the spread of Human culture, language and genetics
 through Europe and Asia? All these are describable via complex, dynamic
 structured graphs and relationships interacting in non-linear ways.
 They are all big enough and complex enough that we should be able to find
 some sort of generic mesoscale statistical-mechanical theory for them.
 Drunk people at parties say things like 
\begin_inset Quotes eld
\end_inset

oh, is a lot like the Ising Model
\begin_inset Quotes erd
\end_inset

 and everyone shakes their heads in agreement, but it never gets farther
 than that.
\end_layout

\begin_layout Standard
This diary entry will start very very small.
 The above grandiose and almost bombastic introduction is meant to serve
 as a reminder to myself as to why I should invest my time and energy into
 this.
 I tend to get distracted easily, and leave projects unfinished.
 The above captures the inspiration for why, perhaps, this diary is needed.
 Again, what follows will be minuscule, compared to the over-arching sketch
 above.
\end_layout

\begin_layout Standard
Here's how to start.
 Consider first Moreau's Necklace-counting function.
 This is a combinatoric function, counting how many distinct necklaces can
 be made from colored beads.
 The beads can be freely arranged; the length and the number of colors are
 fixed.
 What's the entropy? For two colors and one dimension, this is just the
 Ising model, with zero interaction.
 What if we imagine the beads to be words, selected from a vocabulary of
 size N, but require that the constructed necklaces be grammatically correct
 sentences? That is, we apply syntactic constraints to limit acceptable
 sentences.
 To make it more physics-like, we should say that grammatically unacceptable
 sentences are high-energy, while grammatical ones are low-energy.
 We can assign to each rule of grammar, each grammatical dependency, a cost,
 an interaction-energy, so that an analogy can be made to ferromagnetic
 and antiferromagnetic behaviors in Ising models.
 This diary is an effort to mash up and push on this collection of ideas.
 I fear it won't get far.
 Every journey starts with a single step, but some of us walk faster and
 more gracefully than others.
\end_layout

\begin_layout Standard
From here on out, you will read a mish-mash of formulas, confusing shit,
 unwarranted leaps of inference, flat-out wrong statements, not-even-wrong
 statements, and hopefully maybe a grand insight.
 Fingers crossed.
 This will probably be written in chronological order, rather than in conceptual
, pedagogical order.
 This will be mostly a diary, with stitching.
\end_layout

\begin_layout Section
Ingredients
\end_layout

\begin_layout Standard
Lets review the proposed ingredients.
 These are combinatorics, statistical mechanics and syntax.
\end_layout

\begin_layout Subsection
Moreau's Necklace-counting Function
\end_layout

\begin_layout Standard
Moreau's neckalce-counting function is well-described in the Wikipedia article
 on the 
\begin_inset CommandInset href
LatexCommand href
name "Necklace Polynomial"
target "https://en.wikipedia.org/wiki/Necklace_polynomial"
literal "false"

\end_inset

, I wrote most of the Wikipedia articles on the Free Lie Algebra, and on
 Hall Words.
 Check the edit history, I am 
\begin_inset CommandInset href
LatexCommand href
name "User 67.198.37.16"
target "https://en.wikipedia.org/wiki/User_talk:67.198.37.16"
literal "false"

\end_inset

.
 I also wrote a chunk of the Poincare-Birkoff-Witt theorem article, and
 the universal enveloping algebra article, and the tensor algebra article,
 and so on.
 So all of those are percolating in the back of my mind as I write this.
 I write this because they percolate, but fail to make an impact.
 I don't know why.
\end_layout

\begin_layout Standard
Necklaces are about beads, but I want to think about words drawn from a
 vocabulary.
 so ideas like cyclic permutations fall by the wayside.
 I guess I want aperiodic necklaces.
\end_layout

\begin_layout Standard
This is probably the wrong place to start.
 Perhaps 
\begin_inset CommandInset href
LatexCommand href
name "Pólya theory"
target "https://en.wikipedia.org/wiki/P%C3%B3lya_enumeration_theorem"
literal "false"

\end_inset

 is more appropriate, and I suspect part of my problems is that I'm ignorant
 of Pólya theory.
 Except I'm only marginally interested in permutations.
\end_layout

\begin_layout Standard
I'm going to skip quoting explicit formulas, until I figure out how to motivate
 their need.
\end_layout

\begin_layout Subsection
Ising model
\end_layout

\begin_layout Standard
I've lightly edited various Wikipedia articles in the general area of the
 
\begin_inset CommandInset href
LatexCommand href
name "Ising model"
target "https://en.wikipedia.org/wiki/Ising_model"
literal "false"

\end_inset

.
 This area has a huge bundle of nearby research as well, of which I am mostly
 ignorant.
 Sorry.
 In the most basic form, we have a lattice 
\begin_inset Formula $\Lambda$
\end_inset

 with lattice sites 
\begin_inset Formula $k\in\Lambda$
\end_inset

.
 At each site 
\begin_inset Formula $k$
\end_inset

 there is a spin 
\begin_inset Formula $\sigma_{k}$
\end_inset

 taking values over some alphabet 
\begin_inset Formula $V$
\end_inset

.
 By convention, 
\begin_inset Formula $V=\left\{ +1,-1\right\} $
\end_inset

 so that 
\begin_inset Formula $\sigma_{k}\in\left\{ +1,-1\right\} $
\end_inset

 is interpreted as spin-up and spin-down.
 The Hamiltonian is
\begin_inset Formula 
\[
H\left(\sigma\right)=-\sum_{\left\langle ij\right\rangle }J_{ij}\sigma_{i}\sigma_{j}-\mu\sum_{j}h_{j}\sigma_{j}
\]

\end_inset

where 
\begin_inset Formula $J_{ij}$
\end_inset

 is the interaction energy, 
\begin_inset Formula $\left\langle ij\right\rangle $
\end_inset

 denotes nearest neighbors, 
\begin_inset Formula $\mu$
\end_inset

 is the magnetization and 
\begin_inset Formula $h_{j}$
\end_inset

 is the magnetic field at site 
\begin_inset Formula $j$
\end_inset

.
 The unsubscripted 
\begin_inset Formula $\sigma=\left\{ \sigma_{k}\right\} _{k\in\Lambda}$
\end_inset

 is a given fixed spin configuration on the lattice 
\begin_inset Formula $\Lambda$
\end_inset

.
\end_layout

\begin_layout Standard
If the interaction strength is homogeneous, but limited to nearest neighbors
 only, then one writes
\begin_inset Formula 
\[
J_{ij}=\begin{cases}
J & \mbox{if \ensuremath{i,j} are nearest neighbors}\\
0 & \mbox{otherwise}
\end{cases}
\]

\end_inset

so that the interaction no longer depends on the sites 
\begin_inset Formula $i,j$
\end_inset

.
 Likewise, 
\begin_inset Formula $h_{j}=h$
\end_inset

 is used to denote a uniform magnetic field.
 This can be folded into 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Subsection
Link Grammar
\end_layout

\begin_layout Standard
For a linguistic application, this is to be generalized by an alphabet 
\begin_inset Formula $V$
\end_inset

 the vocabulary, with the size of the vocabulary 
\begin_inset Formula $\left|V\right|$
\end_inset

 being ten thousand or 100 thousand words, or perhaps a mixture of words
 and morphemes.
 In transformer theory, this is called 
\begin_inset Quotes eld
\end_inset

subword tokenization
\begin_inset Quotes erd
\end_inset

, a variant of which is 
\begin_inset Quotes eld
\end_inset

Byte Pair Encoding
\begin_inset Quotes erd
\end_inset

.
 To stick to more classical linguistic theory, I like calling them 
\begin_inset Quotes eld
\end_inset

morphemes
\begin_inset Quotes erd
\end_inset

, even though that invites arguments.
 The arguments are evaded with probability.
\end_layout

\begin_layout Standard
The intended Hamiltonian is as follows.
 I will need a few attempts to get this right, so bear with me.
\begin_inset Formula 
\begin{equation}
H\left(\sigma\right)=-\sum_{l}C_{l}\left(\sigma\right)\label{eq:sum over linkages}
\end{equation}

\end_inset

where 
\begin_inset Formula $\sigma$
\end_inset

 is now a fixed sequence of words (of some fixed length) and 
\begin_inset Formula $C_{l}\left(\sigma\right)$
\end_inset

 is the total cost of the linkage 
\begin_inset Formula $l$
\end_inset

 of those words.
 The 
\begin_inset Quotes eld
\end_inset

cost
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

linkage
\begin_inset Quotes erd
\end_inset

 are terminology from Link Grammar.
 Linkages factorize in such a way that only certain words interact, but
 not others, in a standard dependency grammar fashion.
 Thus intransitive verbs link to subjects, but transitive verbs require
 both subjects and objects.
 I won't recap the theory, read the papers.
 The primary property is that, for a fixed word-sequence 
\begin_inset Formula $\sigma$
\end_inset

 and a fixed linkage 
\begin_inset Formula $l$
\end_inset

, the cost 
\begin_inset Formula $C_{l}\left(\sigma\right)$
\end_inset

 factorizes linearly, as
\begin_inset Formula 
\begin{equation}
C_{l}\left(\sigma\right)=\sum_{d}J_{d}\left(w_{i(1)},w_{i(2)},\cdots,w_{i(m)}\right)\label{eq:sum over disjuncts}
\end{equation}

\end_inset

where 
\begin_inset Formula $\sigma=\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 is the sequence of words 
\begin_inset Formula $\sigma_{k}=w_{k}$
\end_inset

 in a sentence of length 
\begin_inset Formula $n$
\end_inset

.
 The sentence consists of a covering by subsequences 
\begin_inset Formula $\left(w_{i(1)},w_{i(2)},\cdots,w_{i(m)}\right)$
\end_inset

 of variable size 
\begin_inset Formula $m$
\end_inset

.
 These subsequences overlap (must overlap) and must cover the entire sentences
 (else a cost of infinity can be imposed).
 That is, every work belongs to one or more subsequences.
 The 
\begin_inset Formula $i(k)$
\end_inset

 is an awkward notation to denote the selected words in the subsequence,
 which I will use till I find a better notation.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $d$
\end_inset

 is a 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

 stemming from the fact that, in Link Grammar, connectors are conjoined
 into one disjunct, and the grammar rules are disjoined collections of disjuncts.
 All that matters here is that 
\begin_inset Formula $J_{d}\left(w_{i(1)},w_{i(2)},\cdots,w_{i(m)}\right)$
\end_inset

 is a real number assigned to a subsequence of words.
 The total cost is the cost of the subsequences.
\end_layout

\begin_layout Standard
Hopefully, the relationship to the Ising model is clear.
 The Ising model has two words, denoted by 
\begin_inset Formula $\uparrow$
\end_inset

 and 
\begin_inset Formula $\downarrow$
\end_inset

.
 It has a set of four disjuncts: 
\begin_inset Formula $\left\{ J_{\uparrow\uparrow},J_{\uparrow\downarrow},J_{\downarrow\uparrow},J_{\downarrow\downarrow}\right\} $
\end_inset

 that have the numerical values 
\begin_inset Formula $J_{\uparrow\uparrow}=J_{\downarrow\downarrow}=+J$
\end_inset

 and 
\begin_inset Formula $J_{\uparrow\downarrow}=J_{\downarrow\uparrow}=-J$
\end_inset

.
 For Link Grammar, the 
\begin_inset Formula $J_{d}$
\end_inset

 might be over nearest neighbors, or they might range over three words,
 and rarely four or more.
 Usually but not always, these interactions are local.
\end_layout

\begin_layout Standard
A multitude of comments:
\end_layout

\begin_layout Itemize
There are many other ways of talking about grammar and syntax.
 They are all, in some general sense, equivalent.
 I like Link Grammar because it is amenable to this kind of thinking.
 This is the incorrect place to demonstrate equivalence between different
 grammatical approaches.
\end_layout

\begin_layout Itemize
In the past, I've claimed that the Link Grammar costs should be thought
 of as entropy or mutual information.
 But for the above model, its clear they should be interpreted as energy.
 I don't yet know how to resolve these conflicting claims.
\end_layout

\begin_layout Itemize
Posing this problem in this way already makes in intractable, since the
 complexity of language means that there cannot be any simple closed-form
 solution.
 I had vaguely thought I'd take a few logarithms of Moreau's necklace counting
 function and find some simple edifying example, but this can't happen in
 this way.
\end_layout

\begin_layout Itemize
Posing the problem in this way does allow numerical treatments, although
 we haven't specified how to obtain the disjunct costs 
\begin_inset Formula $J_{d}$
\end_inset

 for a natural language.
 The original Link Grammar does provide a dictionary of these, but that
 dict is hand-built.
 I've explored learning algos for it, but will not get into that here.
\end_layout

\begin_layout Itemize
Posing the problem this way does allow reformulation as a graph-theoretic
 maximum-cut problem.
 See the Wikipedia article for the 
\begin_inset CommandInset href
LatexCommand href
name "Ising model"
target "https://en.wikipedia.org/wiki/Ising_model"
literal "false"

\end_inset

.
 For AI, the maximum cut can be tied to Tononi's 
\begin_inset Quotes eld
\end_inset

cruelest cut
\begin_inset Quotes erd
\end_inset

, and Tegmark's exposition of the same.
 (Tegmark, 
\begin_inset CommandInset href
LatexCommand href
name "Consciousness as a State Of Matter"
target "https://www.sciencedirect.com/science/article/abs/pii/S0960077915000958"
literal "false"

\end_inset

, 2015)
\end_layout

\begin_layout Itemize
The point of writing down the 
\begin_inset Formula $J_{d}$
\end_inset

 is that it is a factorization of what would otherwise be an 
\begin_inset Formula $n$
\end_inset

–word interaction 
\begin_inset Formula $C_{l}\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
This last point is the pebble in my shoe.
 The conventional deep-learning neural net transformers work with unfactored
 
\begin_inset Formula $C_{l}\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 directly, working against the intuition of linguists that such factorization
 is possible.
 I've repeatedly hand-waved about this, but never seriously sat down to
 articulate this precisely.
 In some sense, the world doesn't care if this is done; they're happy with
 transformers as they are.
 Myself, I'm irked, but I make only lame attempts to extract that pebble
 from my shoe.
\end_layout

\begin_layout Subsection
Statistical Mechanics
\end_layout

\begin_layout Standard
For the present case, the Boltzmann distribution suffices.
 It is
\begin_inset Formula 
\[
P_{\beta}\left(\sigma\right)=\frac{\exp-\beta H\left(\sigma\right)}{Z_{\beta}}
\]

\end_inset

with 
\begin_inset Formula $\beta$
\end_inset

 the inverse temperature, and 
\begin_inset Formula $Z_{\beta}$
\end_inset

 the partition function,
\begin_inset Formula 
\[
Z_{\beta}=\sum_{\sigma}\exp-\beta H\left(\sigma\right)
\]

\end_inset

More precisely, the above should be taken to be the 
\begin_inset CommandInset href
LatexCommand href
name "Gibbs measure"
target "https://en.wikipedia.org/wiki/Gibbs_measure"
literal "false"

\end_inset

.
 Writing the energy as a sum over linkages 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sum over linkages"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and sum over disjuncts 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sum over disjuncts"
plural "false"
caps "false"
noprefix "false"

\end_inset

 imply that the probability factorizes into Markov cluster.
 The resulting form goes under the name of a 
\begin_inset CommandInset href
LatexCommand href
name "Markov random field"
target "https://en.wikipedia.org/wiki/Markov_random_field"
literal "false"

\end_inset

.
 The Wikipedia article provides a suitable notation.
 When presented in the form of a 
\begin_inset CommandInset href
LatexCommand href
name "conditional random field"
target "https://en.wikipedia.org/wiki/Conditional_random_field"
literal "false"

\end_inset

, the utility for natural language and machine learning has already been
 recognized: Wikipedia gives papers dating to 2001, one by John Lafferty,
 a co-creator of Link Grammar.
\end_layout

\begin_layout Standard
The question mark, or pebble in my shoe, is whether and how it might be
 possible to factorize transformers, in all their varied manifestations,
 into Markov random fields.
 Again, linguistics suggests this should be possible, at least to some approxima
te degree.
 Good old-fashioned AI also suggests this.
 But what is that factorization?
\end_layout

\begin_layout Subsection
Geometry
\end_layout

\begin_layout Standard
I wish to give the Gibbs measure a differential–geometric interpretation.
 The inspiration would be the use of exp to map from adjoint Lie algebras
 to elements of the Lie group, or perhaps more generally, the use of exp
 to map from vectors in a tangent space to geodesics in Riemannian manifolds.
 This is the intended inspiration.
 
\end_layout

\begin_layout Standard
My gut sense is that this is possible, and that surely, some variation of
 this must have been articulated before.
 I am ignorant of the literature.
 Perhaps Gaussian orthogonal ensembles play into this.
 Perhaps results from Sherrington–Kirkpatrick models of spin glasses.
 I don't know.
 So I have to reinvent from scratch.
 And do it without foundering.
\end_layout

\begin_layout Standard
There are several possible approaches.
 One is to notices that its after midnight, and time to got to bed.
 That's perhaps the preferred approach.
\end_layout

\begin_layout Standard
We take the vocabulary 
\begin_inset Formula $V$
\end_inset

 to define a high-dimensional vector space.
 Each word is identified with a single basis element of that vector space.
 
\end_layout

\begin_layout Section
The End
\end_layout

\end_body
\end_document
