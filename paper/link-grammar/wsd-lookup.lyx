#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass IEEEtran
\use_default_options false
\language english
\inputencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\float_placement tbh
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 0
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Table-driven Word Sense Disambiguation
\end_layout

\begin_layout Author
Linas Vepstas
\begin_inset Foot
status open

\begin_layout Plain Layout
Linas Vepstas is with Novamente LLC
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
The age-old observation that sense and syntax are correlated is used to
 build a table-driven word-sense disambiguation algorithm.
 Because the sense is determined by a table lookup, it is extremely fast,
 making it suitable for practical, real-time semantic-web applications.
 By contrast, existing word-sense disambiguation (WSD) or semantic role
 labelling (SRL) algorithms are fairly CPU-intensive, requiring many seconds
 or even minutes to run on modern hardware.
 
\end_layout

\begin_layout Abstract
The lookup table is constructed by correlating natural language parse structures
 with word-sense assignments.
 The key observation is that certain parse structures resemble very fine-grained
 part-of-speech (POS) tags, and that this fine-grained tag can be used to
 construct a more fine-grained (but otherwise traditional) word-sense lexicon.
 Manually creating such a dictionary is overwhelming, and so the lexicon
 is constructed using automated techniques: a large body of text is tagged
 with senses using a standard WSD algorithm, and then tabulated with the
 result of NLP parses of the same text.
 The POS distinctions are much finer than the usual noun/verb/adj/adv distinctio
ns or even the Penn Treebank POS tags.
 These fine-grained tags allow the correlation between sense and syntax
 to be examined at a more detailed level than usual.
\end_layout

\begin_layout Keywords
Lexicon, NLP, parsing, word-sense disambiguation, WSD, SRL, Semantic role
 labelling, Link Grammar
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
That there is a correlation between English-language syntax, and word senses,
 is obvious, and underlies the presentation of dictionaries: word senses
 are traditionally grouped according to part-of-speech (POS): noun, verb,
 adjective, adverb.
 Parts of speech are crude indicators of the allowed syntactic use of a
 word: so, in general, one cannot use a noun in place of a verb.
 Of course, there are exceptions: English grammar does allow the use of
 nouns as noun modifiers, the verbing of nouns by use of gerunds, and one
 can, to a limited extent, verbize one's nouns directly.
 But each of these grammatical operations alters the sense of a word as
 well as changing its POS category; the word sense is no longer the same,
 and most dictionaries supply distinct entries for each usage: senses and
 POS tags correlate.
\end_layout

\begin_layout Standard
Some dictionaries attempt a more fine-grained distinction of word senses:
 thus, for example, The American Heritage® Dictionary of the English Language
\begin_inset CommandInset citation
LatexCommand cite
key "AmHeritage2000"

\end_inset

 frequently uses section headings of 
\emph on
v.tr.

\emph default
 and 
\emph on
v.intr.

\emph default
 to denote word senses that are used only with the transitive or intranstive
 form of a verb.
 Thus, upon witnessing a verb used in a certain sentence in a certain way,
 one can immediately narrow down the possible senses for that verb.
\end_layout

\begin_layout Standard
It is is natural to presume that the correlation between word senses and
 word usage might continue to a finer, more detailed level, if only one
 could make finer, more detailed syntactic observations, and had a finer,
 more detailed lexicon to work with.
 This is the primary thesis of this paper: such finer distinctions are possible
 and useful.
 To illustrate this, consider, for example, the verb 
\begin_inset Quotes eld
\end_inset


\emph on
to suffer
\emph default

\begin_inset Quotes erd
\end_inset

.
 The American Heritage Dictionary lists seven senses for this; other dictionarie
s list from 5 to 11 senses.
 The Princeton WordNet dictionary
\begin_inset CommandInset citation
LatexCommand cite
key "Wordnet1998"

\end_inset

 lists 11 meanings.
 Now consider the sentence:
\end_layout

\begin_layout Quotation

\emph on
\begin_inset Quotes eld
\end_inset

She suffered a fracture in the accident.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
A syntactic analysis shows that 
\begin_inset Quotes eld
\end_inset


\emph on
suffer
\emph default

\begin_inset Quotes erd
\end_inset

 is in the past tense, takes a singular pronomial subject, a singular direct
 object, and a modifying prepositional phrase.
 The corresponding WordNet sense key is 
\emph on
suffer%2:29:01::, (undergo (as of injuries and illnesses))
\emph default
.
 This example is syntactically quite different than those of other example
 sentences taken from WordNet:
\end_layout

\begin_layout Quotation

\emph on
\begin_inset Quotes eld
\end_inset

This author really suffers in translation.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
where the verb is in the present tense, has a singular subject, a prepositional
 modifier, but no object: it corresponds to sense 
\emph on
suffer%2:30:02::, (be set at a disadvantage)
\emph default
.
 Similarly one has
\end_layout

\begin_layout Quotation

\emph on
\begin_inset Quotes erd
\end_inset

Many saints suffered martyrdom.
\emph default

\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
which has a plural subject, is in past tense, and the object is an uncountable
 (mass) noun: this corresponds to the sense 
\emph on
suffer%2:39:01::, (undergo or be subjected to)
\emph default
.
 Each of these example sentences, taken from WordNet itself, shows a strong
 correlation between the syntactic structure of the sentence, and the designated
 word sense.
 
\end_layout

\begin_layout Standard
Most of this correlation can be erased by altering the example sentences,
 and creating new, grammatically correct sentences, by altering the tense,
 the count of the subject or object, the presence or absence of the object,
 or adding/removing a modifying perpositional phrase.
 But do native English speakers generate all possible sentence constructions
 with equal probability? Clearly not: idioms, the strict coupling of a few
 words, are a well-known liguistic phenomenon.
 This observation can be taken much farther: the Mel'cuk Meaning-Text Theory
\begin_inset CommandInset citation
LatexCommand cite
key "Mel'cuk1987,Steele1990"

\end_inset

 posits that the intended meaning of an expression strongly influences the
 structure of the sentence that expresses that meaning; conversely, that
 certain sentence patterns are used only with limited, well-defined subsets
 of nouns, verbs.
 Thus, the proper study of the correlation between sense and syntax, in
 order to be valid and correct, must become an exercise in corpus linguistics:
 the tabulation of the frequency of word usage as used by actual writers,
 as compared to the intended sense of the author.
 To the extent that such a correlation exists, it can be tabulated into
 a dictionary, and this dictionary can, in turn, be used to quickly guess
 at the intended meaning, given only the syntactic usage.
\end_layout

\begin_layout Standard
Discovering this correlation relies on large-scale computational linguistics
 techniques.
 Although one might be able to analyze, by hand, a few verbs or nouns, in
 the manner illustrated above, this is hardly practical to get a true sense
 of the prevelance of this correlation.
 In doing a small-scale analysis, one might discover only a few words that
 appear to be highly correlated, without being able to say much about the
 language, in general.
 What is wanted is a full lexicon that makes fine-grained POS distinctions.
 Its not practical to build such a lexicon by hand; it must be constructed
 in some automated way.
 The approach used here is to parse a large quantity of text, obtaining
 syntactic structure, and to tag the same text with word-senses, and so
 as to generate statistics correlating the two tag sets.
 
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

weak link
\begin_inset Quotes erd
\end_inset

 to this automated approach is sense tagging.
 The quantity of text hand-tagged by experts with word senses is not large
 enough to usefule for collecting a reasonable amount of statistics; thus
 sense-tagging must be performed automatically.
 Although automated sense tagging or 
\begin_inset Quotes eld
\end_inset

word sense disambiguation
\begin_inset Quotes erd
\end_inset

 (WSD) algorithms have been improving, the best simple, straight-forward
 systems remain marginal in precision and recall.
 This raises an interesting question: is it possible that, by correlating
 sense tags with syntactic usage, that the incorrect tags will statistically
 
\begin_inset Quotes eld
\end_inset

cancel out
\begin_inset Quotes erd
\end_inset

, while correct tags will reinforce? That is, is it possible that a lookup-table
 based WSD algorithm might have euqal or better accuracy than the underlying
 WSD system from which it was constructed? 
\end_layout

\begin_layout Standard
For parsing, the Link Grammar parser
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"

\end_inset

 is used.
 The output of the Link Grammar parser are 
\begin_inset Quotes eld
\end_inset

linkages
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

disjuncts
\begin_inset Quotes erd
\end_inset

 that explicitly express how a word is connected to its vicinity.
 Each disjunct is essentially a linkage statement, such as 
\begin_inset Quotes eld
\end_inset

this is a verb with a singular subject on the left, a singular direct object
 on the right, and a modifying prepositional phrase
\begin_inset Quotes erd
\end_inset

, expressed in a compact notation (
\begin_inset Quotes eld
\end_inset


\emph on
Ss- Os+ MVp+
\emph default

\begin_inset Quotes erd
\end_inset

 for this example).
 These disjuncts are taken as the 
\begin_inset Quotes eld
\end_inset

fine-grained POS tags
\begin_inset Quotes erd
\end_inset

 refered to above.
 In certain ways, Link Grammar resembles dependency parsing, and indeed,
 it is straight-forward to convert a Link Grammar parse into a dependency
 parse
\begin_inset Foot
status open

\begin_layout Plain Layout
The RelEx semantic relationship extractor[[XXX need ref] accepts Link-Grammar
 parses as input, and, among other outputs, can generate dependency parses
 fully compatible with the well-known Stanford dependency parser.
\end_layout

\end_inset

.
 Unlike a dependency parse, Link Grammar does not explicitly indicate head
 words; but this is not required for the notion of fine-grained POS tags.
 Conversely, it is straight-forward to create a 
\begin_inset Quotes eld
\end_inset

fine-grained POS tag
\begin_inset Quotes erd
\end_inset

 from a dependency parse; thus, the results in this paper should be reproducible
 by employing other parsers with dependency output.
 It is less clear how to extract a 
\begin_inset Quotes eld
\end_inset

fine-grained POS
\begin_inset Quotes erd
\end_inset

 from a phrasal or phrase-structure parse: phrasal parses do not indicate
 the relationships and constraints between words; rather, the production
 rules of such parsers contain many non-terminal (non-word) symbols.
 Perhaps the sequence of production rules taken to arrive at a given word
 could serve to act as a 
\begin_inset Quotes eld
\end_inset

fine-grained POS tag
\begin_inset Quotes erd
\end_inset

 for that word.
 Understanding how different parsers change the the correlation between
 POS and sense tags may shed light into the nature of the syntactic/semantic
 connection.
 This question is not explored in this paper.
\end_layout

\begin_layout Standard
Word-sense tagging was performed using an implementation of the Mihalcea
 all-words word-sense disambiguation algorithm
\begin_inset CommandInset citation
LatexCommand cite
key "Mihalcea2004,Mihalcea2005,Mihalcea2007"

\end_inset

.
 This algorithm is reasonably accurate and is straightforward to implement.
 It requires the use of a word-sense similarity measure; several such measures
 available for WordNet.
 The output of the Mihalcea WSD is a ranked list of word-sense assignments
 (WordNet senses) for each word in a text.
 The final dicitionary created is then a tabulation of the frequency with
 which a given Link-Grammar disjunct was observed with a given WordNet sense
 key.
\end_layout

\begin_layout Standard
It is important to recognize that table-driven patten recognition is a biologica
lly plausible model for human cognition.
 Pattern recognition is a central precept of the connectionist philosophy
 of human cognition [xxx need ref].
 It seems unlikely that the human brain implements proceedural computational
 algorithms; rather, connectionism states that symbol manipulation is performed
 with neural-net style architectures.
 A table-lookup based pattern recognizer fits neatly into this picture:
 one might imagine a single neuron with a set of connections sensitive to
 a particular syntactic pattern; when that pattern is presented on the input,
 the neuron fires.
 In this sense, the table lookup of a pattern resembles a single-layer discrimin
atory neural net or perceptron; for a given input, only a small number of
 outputs (word senses) are suggested.
 This analogy indicates the power as well as the weakness of the tehnique.
 Table lookup can be made massively parallel: those neurons that know of
 a specific pattern respond, and all others are silent.
 This echoes the massively-parallel structure of the brain.
 But single-layer perceptrons are also notoriously limited: they can classify
 
\begin_inset Quotes eld
\end_inset

linerarly seperable
\begin_inset Quotes erd
\end_inset

 patterns, but no more.
 There is no doubt that language is far more complex than that.
 There is no reason to believe that table-driven WSD could ever function
 more accurately than a single-layer perceptron: its strength is speed,
 and a certain degree of 
\begin_inset Quotes eld
\end_inset

biological naturalness
\begin_inset Quotes erd
\end_inset

.
 One does not expect exquisite accuracy.
 In the connectionist philosophy, greater sophistication requires that the
 output of one layer be fed to the input of another: thus, for example,
 table-driven WSD might provide a-priori weighted word-sense suggestions
 for other, more refined semantic algorithms.
\end_layout

\begin_layout Standard
The mechanism used to construct the lexicon is also fairly connectionist
 in nature.
 The Mihalcea all-words WSD graph algorithm resembles a Markov chain, solved
 using the PageRank algorithm, and is thus fairly explicitly connectionist
 in and of itself.
 The Link-Grammar parser itself is currently implemented using a computational,
 not connectionist algorithm.
 However, the author beleives that it should be possible to re-implement
 the parser using the Viterbi algorithm.
 This could provide a significant speedup, especially for long sentences,
 as well as making the parser more biologically natural: the Viterbi algorithm
 is explicitly connectionist, maintaining only a finite history, with a
 finite number of connections to recent input (i.e.
 enouraging and maintaining mosly just short-distance connections between
 words).
\end_layout

\begin_layout Standard
Breif results summary ...
 
\end_layout

\begin_layout Standard
Outline of paper....
\end_layout

\begin_layout Standard
use 
\begin_inset Quotes eld
\end_inset

semantic role labelling
\begin_inset Quotes erd
\end_inset

 in a few more paras.
\end_layout

\begin_layout Section
Previous Work
\end_layout

\begin_layout Standard
Perhaps the most developed of linguistic theories that attempt to describe
 how syntax gives rise to semantics is Igor Mel’čuk's 
\begin_inset Quotes eld
\end_inset

Meaning-Text Theory
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Steele1990,Mel'cuk1987"

\end_inset

.
 The theory describes a network of relationships between semantic lexemes
 and a set of functions and proceedures that relate this network to correspondin
g grammatically correct utterances.
 In particular, the theory attempts to explain why a 
\begin_inset Quotes eld
\end_inset

phraseme
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

set phrase
\begin_inset Quotes erd
\end_inset

, such as the phrase 
\begin_inset Quotes eld
\end_inset


\emph on
to give X a look
\emph default

\begin_inset Quotes erd
\end_inset

, constricts the kind of things that can occupy the slot 
\emph on
X
\emph default
.
 In this example, 
\emph on
X
\emph default
 typically names a process, a product, an idea or proposal.
 A key observation in MTT is that, semantically, 
\emph on
X
\emph default
 cannot be just 
\begin_inset Quotes eld
\end_inset

any old noun
\begin_inset Quotes erd
\end_inset

; even though pure syntactic analysis would allow this.
 To fill this slot in a semantically meaningful way, a noun would have to
 name or denote something worth pondering, reviewing, looking over.
 One of the acheivements of the theory is to precisely specify how such
 a restriction comes about and how it can be maintained in a computational
 way.
 
\end_layout

\begin_layout Standard
The structure of an MTT 
\begin_inset Quotes erd
\end_inset

Explanatory Combinatory Dictionary
\begin_inset Quotes erd
\end_inset

 is quite different than an ordinary dictinary: it gives precise and detailed
 instructions on how to convert lexical meaning into syntactic expression.
 In this sense, it correlates syntax and semantics.
 By contrast, the work described in this paper shows how to automatically
 create a short-cut or abridged form of such an ECD: for any given word-sense,
 one has a table of allowed syntactic structures in which that word sense
 has been observed (and a frequency count of how often that word-sense was
 used in this particular way).
 The MTT ECD is hand-constructed by linguists after hard, patient work.
 The approach described here obtains relationships automatically, with an
 unsupervised learning algorithm.
 
\end_layout

\begin_layout Standard
There are other approaches for the unsupervised learning of syntactic/semantic
 correlations.
 These include Dekang Lin's work on automatic synonym discovery
\begin_inset CommandInset citation
LatexCommand cite
key "Lin1998,Lin2001"

\end_inset

, the work of Domingos and others on applying Markov Logic Networks (MLN)
\begin_inset CommandInset citation
LatexCommand cite
key "Domingos2006"

\end_inset

 to semantics
\begin_inset CommandInset citation
LatexCommand cite
key "Poon2009,Meza-Ruiz2009"

\end_inset

.
 Each of these approaches also have one strength that the current approach
 does not: they potentially discover much narrower classes of words that
 can appear in juxtaposition to each other.
 That is, the current Link Grammar word classes are, for the most part,
 broad, syntactic categories, and not narrow, semantic classes.
 So, for example, the Link Grammar link 
\emph on
Os+
\emph default
, which is short-hand for the statement 
\begin_inset Quotes eld
\end_inset


\emph on
this word takes a singular direct object on the right
\emph default

\begin_inset Quotes erd
\end_inset

, places no particular restriction on what that object might be -- it can
 be any noun.
 This is in contrast to the 
\begin_inset Quotes eld
\end_inset


\emph on
give X a look
\emph default

\begin_inset Quotes erd
\end_inset

 example, where X is syntactically free, but semantically constrained.
\end_layout

\begin_layout Standard
Lin describes how to automatically build a thesaurus in 
\begin_inset CommandInset citation
LatexCommand cite
key "Lin1998"

\end_inset

, by examing the frequency with which words are used in similar dependency
 relations.
 Specifically, he creates a dependency parse, having relations of the form
 
\begin_inset Formula $r(w_{1},w_{2})$
\end_inset

 with 
\begin_inset Formula $r$
\end_inset

 a relation (such as 
\emph on
subj
\emph default
, 
\emph on
obj
\emph default
, 
\emph on
etc
\emph default
.), and 
\begin_inset Formula $w_{1}$
\end_inset

 and 
\begin_inset Formula $w_{2}$
\end_inset

 are words.
 By examing co-occurance statistics for such relations in a large corpus,
 he provides a number of similarity measures which are able to successfully
 identify synonymous words.
 This result is quite remarkable.
 It can be criticized in two ways: by collecting statistics on just individual
 relations, it does not look at the broader syntactic context of a word.
 That is, for example, the meaning of a word may depend on its having both
 a direct object, and a prepositional relation, both within the same sentence.
 This is partly corrected in a later work
\begin_inset CommandInset citation
LatexCommand cite
key "Lin2001"

\end_inset

, which groups together dependency paths to discover synonymmous phrases.
 A similar solution is explored in the Markov logic work described below.
 Either approach provides for the automatic discovery of significant parts
 of an entry of the ECD dictionary of MTT theory.
 A second criticism is that the work does not suggest how to discover that
 a given word may have multiple meanings: it does not specify how to group
 synonyms into synonym sets.
 As such, there is no direct way to extend this work to perform sense labelling
 on individual words in a sentence: there is no way to look at a word in
 a sentence and discover the intended lexeme.
\end_layout

\begin_layout Standard
A global approach to automatic synonymous phrase discovery is offered by
 applying the theory of Markov logic networks to the extraction of semantic
 content from a dependency parse
\begin_inset CommandInset citation
LatexCommand cite
key "Poon2009"

\end_inset

.
 The approach used extracts, tabulates and clusters sentence patterns according
 to common substructures, thus automatically discovering synonymous expressions
 by performing a global sentence analysis.
 In certain ways, it can be thought of as a generalization of Lin's DIRT
\begin_inset CommandInset citation
LatexCommand cite
key "Lin2001"

\end_inset

.
 The system is remarkable in that it is completely unsupervised (does not
 require a training corpus), and requires very few 
\emph on
a priori 
\emph default
built in rules.
 Although it significantly outperforms other question answering systems,
 this result can also be taken as a critique: it does not explicitly provide
 markup or tagged output; but rather performs question answering by pattern
 matching.
\end_layout

\begin_layout Standard
A contrasting application of Markov logic nets is that of Meza-Ruiz, 
\emph on
et al
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "Meza-Ruiz2009"

\end_inset

.
 This system uses MLN to simultaneously provide a variety of tags, and,
 in particular, to identify the head-word of a sentence, disambiguate the
 semantic frame that it participates in, and provide a semantic role tag
 (essentially, a word-sense) for the head-word.
 Unfortunately, the system uses a large number of hand-crafted relations,
 frames and rules to specify the logic network.
 However, once trained, the weighted network can be quickly evaluated to
 obtain tags for any given sentence, typically in fractions of a second
 for contemporary computers.
\end_layout

\begin_layout Standard
It is an interesting exercise to contrast the above-descibed global sentence
 analysis systems to the current work in dependency parsing, such as minimum
 spanning tree approaches
\begin_inset CommandInset citation
LatexCommand cite
key "McDonald2006"

\end_inset

 or greedy state machines
\begin_inset CommandInset citation
LatexCommand cite
key "Nivre2006"

\end_inset

, where global structure is essentially ignored, except to constrain the
 parse to include all of the words in a sentence.
 In essence, syntax is a local constraint on word juxtapositions; semantics
 is a global anlysis of word relationships.
 This fits well into the theoretical framework of Meaning-Text Theory; that
 the current work lies in a middle ground is perhaps no accident.
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
describe link-grammar disjuncts.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
what about results by POS category?? e.g.
 does this work better for verbs, or for nouns? wahat about adj/adv?
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\family sans
A single column figure goes here
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Captions go 
\emph on
under
\emph default
 the figure
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Table captions go 
\emph on
above
\emph default
 the table
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features>
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
delete
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
this
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
example
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
table
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
cconclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/home/linas/src/fractal/paper/lang"
options "plain"

\end_inset


\end_layout

\begin_layout Biography
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

Your Name
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 All about you and the what your interests are.
 Don't forget to put your name in between a pair of {}'s that are set as
 raw TeX.
\end_layout

\begin_layout --Separator--

\end_layout

\end_body
\end_document
